大O表示法指出了算法运行时间的增速，说的是最糟糕的情景

从快到慢的顺序列出了你经常会遇到的5种大O运行时间

O(log n)

O(n)

O(n*log n)

O(n^2^)

O(n!)



数组与链表的对比。

本质区别：数组存储大小固定，位置紧靠；链表存储大小可调，位置随机，下一个位置存储在上一个元素中。

而这样的区别意味着链表在读取所有元素时效率很高，同时插入元素很方表；数组在随机访问时效率很高。

<img src="D:\research_workshop_new\算法\learn\算法.assets\image-20210310213443678.png" alt="image-20210310213443678" style="zoom: 33%;" />



递归

递归只是让解决方案更清晰，并没有性能上的优势，实际上，使用循环的性能更好。

使用递归时必须要告诉程序什么时候结束。这个停止递归的条件称为基线条件。



栈和调用栈

栈有压入和弹出，而我们调用函数相当于我们压入一个函数，而弹出时后压入的先弹出，栈帮助你跟踪函数，但是同时使用的内存也比较大了。

分而治之(divide and conquer)是一种解决递归问题的思想方法。

工作原理就是1.找出简单的基线条件    2.确定如何缩小问题的规模，使其符合基线条件。

##快速排序##

快速排序内容比较多，这里只记录关键信息，看不明白再去看算法图解。

快速排序是典型的分而治之的应用。

步骤如下：

1.选择基准值

2.将数组分为两个子数组，小于基准值和大于

3.对这两个子数组进行快速排序也就是再从1开始。

调用栈高度不一样导致存在最好情况和最差情况

快速排序平均运行时间是O(n) * O(log n) (这部分是调用栈高度)= O(n log n)



此外在这部分还讲了大O计时法。不考虑常量是因为如果两种算法的大O运行时间不同，常量不重要。

但有时候时间常量有时候不能忽略-例如快速查找和合并查找，快速查找常量比合并查找小，如果运行时间都是O(n log n)，那快速查找更快。

##散列表##

散列表是基本数据结构之一

散列函数完成的作用是将输入映射为数字，最好是不同的数字。

散列函数知道数组有多大，只返回有效的索引，如果数组包含5个元素，散列函数就不会返回无效索引100。

散列表使用散列函数来确定元素的存储位置。

获取元素的速度和数组一样快。

python提供的散列表实现为字典，可以用dict来创建。

散列表常见应用：大海捞针的查找，防止重复



关于图的算法

首先是广度优先搜索算法-----段数最少

每条边考虑了权重，那么就用狄克斯特拉算法，-----总权重最小。需要注意的是此算法无法解决负权重边，

如果有负权重使用贝尔曼-福德算法



NP完全问题

NP完全问题解决速度是非常慢的

元素较少时算法的运行速度非常快 ， 但随着元素数址的增加， 速度会变得非常慢。
1涉及 “所有组合” 的问题通常是NP完全问题。
2 不能将问题分成小问题， 必须考虑各种可能的情况。 这可能是NP完全问题。
3如果问题涉及序列（如旅行商问题中的城市序列）且难以解决， 它可能就是NP完全问题。 

4如果问题涉及集合（如广播台集合）且难以解决， 它可能就是NP完全问题。
5如果问题可转换为集合覆盖问题或旅行商问题， 那它肯定是NP完全问题。



可以使用贪婪算法，贪婪算法是一种近似算法，追求每一步都是最优解，虽然这样和完美解有一些差距，但是差距并不大，而且运行时间大大减小。

